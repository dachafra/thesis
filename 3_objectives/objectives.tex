\chapter{Objectives and Contributions}

\epigraph{As computer scientist the most important part of a research is the what, not the how}{\textit{Maria-Esther Vidal}}

\label{chap:objectives}
As discussed in the previous chapters, in our work we focus on the exploitation of declarative mapping rules for the construction of knowledge graphs from heterogeneous data sources. Additionally, in order to provide a proper and objective evaluation of these kind of systems, we propose and design a set of methodologies and benchmarks to address this issue. This chapter presents the objectives of our work and our contributions to the current state of the art. We also enumerate the assumptions considered when we started this work, and describe the most relevant hypothesis and restrictions that delimit the scope of this thesis.

\section{Objectives}
In the context of this thesis we identify two different goals. First, we propose the concept of \textit{mapping translation} and describe its main properties. Exploiting this concept, and, bringing out a new generation of knowledge graph construction systems, we describe two systems that construct virtual knowledge graphs (\textit{Morph-CSV} and \textit{Morph-GraphQL}) and other two that build materialized KGs at scale (\textit{SDM-RDFizer} and \textit{FunMap}). The second goal of this thesis is to define standard and objective methodologies to evaluate this new generation of KG construction systems. We present a benchmark for virtual knowledge graph access systems \textit{GTFS-Madrid-Bench}, a set of RML test-cases to evaluate the conformance of its engines and an analysis of the impact over the performance of different parameters during the construction of materialized knowledge graphs.

In order to achieve the first objective of this thesis, we need to tackle the following open research problems:

\begin{itemize}
    \item The current heterogeneity in the number of declarative mapping specifications reduces the benefits of this approach for constructing knowledge graphs. Based on the W3C recommendation R2RML~\citep{R2RML}, and with the aim of providing support to other formats beyond relational databases, many new declarative mapping languages are proposed such as RML~\citep{dimou2014rml}, xR2RML~\citep{michel2015translation}, KR2RML~\citep{slepicka2015kr2rml}, CSVW~\citep{tennison2015model}, or D2RML~\citep{chortaras2018mapping}. Together with each specification their corresponding parsers have to be implemented, limiting their scope or generalizable optimizations, which negatively impacts on the global adoption of these data integration systems in real-world applications of knowledge graphs.

    \item The construction of virtual knowledge graphs from heterogeneous data sources (e.g., CSV, JSON, XML or RDB) does not exhibit good query evaluation performance nor completeness. The common technique is to use na\"ive SQL data wrappers that hidden the complexity of the input sources. However, in order to tackle the advantage of proposed SPARQL-to-SQL optimization techniques~\citep{priyatna2014formalisation,calvanese2017ontop}, well-formed RDB instances are required, including their corresponding domain and integrity constraints.

    \item Current approaches for constructing materialized knowledge graphs do not scale-up in complex data integration scenarios (e.g., high rate of duplicates or interoperability issues over the input data sources, high data volume, etc.). The absence of engines to efficiently parse declarative mapping languages to construct materialized knowledge graphs hinders their global and industry adoption over real use cases.
\end{itemize}

The second goal is focused on proposing evaluations of knowledge graph construction systems to understand what are their main current limits, and it has the following open research problems:
\begin{itemize}
    \item The evaluations performed over materialization knowledge graphs engines are not representative neither general enough to provide an overview of the current limitations of the applied techniques. Engines such as RocketRML~\citep{csimcsek2019rocketrml} and SPARQL-Generate~\citep{lefranccois2017sparql} evaluate their features using their own use cases, datasets and parameters. 
    \item The current virtual knowledge graph construction benchmarks~\citep{lanti2015npd,bizer2009berlin} are mainly focused on relational databases. Hence, they are not enough to test the capabilities of the new generation of engines that are able to run or distribute SPARQL queries over data in several formats. 
\end{itemize}



\section{Contributions to the State of the Art}
In this work, we aim to provide solutions to the open research problems described in the previous section. The contributions for solving the first research problem are:

\begin{enumerate}
    \item[\textbf{C1.1.}] \textbf{The concept of \textit{Mapping Translation} and its main properties}. We motivate the idea of making the different mapping language specifications interoperable among them adding a new layer in a knowledge graph construction workflow, where mapping can be translated. Additionally, we define the desirable properties of this concept based on the work of~\citep{hartig2017foundations} and we demonstrate their usability over a set of use cases extracted from the literature where it has been successfully applied.
    \item[\textbf{C1.2.}] The \textbf{formalization and application of constraints extracted from mappings rules and metadata over tabular data} to enhance virtual KG construction approaches. Focused on tabular data sources, we extend a typical virtual KG construction workflow adding a set of additional pre-processing steps to handle common issues for querying tabular data source. Exploiting information from mapping rules and metadata annotations, we are able to efficiently apply integrity and domain constraints, together with a set of new optimizations steps, for enhancing performance but also completeness in the (virtual) KG construction over tabular sources.
    \item[\textbf{C1.3.}] The exploitation of standard \textbf{declarative mapping rules to automate the generation of functional wrappers} for virtual KG construction. Following best practices on data publication on the web~\citep{bizer2011linked} in order to avoid data silos and with the aim to help knowledge and software engineers to design and implement efficient functional wrappers, we propose a solution that exploiting mapping rules for the automatic creation of programmed wrappers. More in detail, we use R2RML mappings and a new interpretation of the SPARQL-to-SQL baseline algorithm presented in~\citep{chebotko2009semantics} for creating programmed and efficient GraphQL wrappers, that allow the translation from GraphQL to SQL queries for obtaining data in a relational databases.
    \item[\textbf{C1.4.}] Definition of \textbf{physical operators exploiting information from mapping rules} for constructing materialized KG at scale. We define a set of data structures  aligned with the different type of tasks that a materialization KGC engine has to perform to create the RDF graph. The data structures, together with their corresponding operators, are able to constructing materialized knowledge graphs in complex data integration scenarios (big data sizes, high rate of duplicates, etc). We demonstrate that our proposal is able to generate KGs at scale in comparison with the state of the art engines. 
    \item[\textbf{C1.5.}] \textbf{Heuristic-based optimizations for functional mappings} (mapping rules with transformation functions) for scaling up the construction of materialized knowledge graphs. Covering the gap of parsing mapping rules that contain ad-hoc transformation functions over the input data sources in a efficient way, we propose a set of mapping and data translation rules to enhance the construction of materialized KGs from this kind of mapping rules.
\end{enumerate}

With regard to the second objective, this work presents new contributions in the following aspects:
\begin{enumerate}
    \item[\textbf{C2.1.}] Definition of a set of \textbf{test cases to evaluate the conformance of KG construction engines in RML}. We extend the proposed R2RML test cases during its process of standardization~\citep{R2RML_test_cases} for covering other kinds of data formats and relational databases. Executed over the RML parsers in its corresponding RML implementation report, they give an overview of the current state of the engines in terms of language conformance.
    \item[\textbf{C2.2.}] Identification and experimental evaluation of the \textbf{parameters that affect the behavior of the KGC engines}. We analyze the typical inputs and their corresponding features in a knowledge graph construction workflow and how they can impact over the performance and completeness of the process. Additionally, we empirically demonstrate the importance of taking into account these parameters for generating representative benchmark and testbeds for these engines.
    \item[\textbf{C2.3.}] A complete and \textbf{representative benchmark for evaluating virtual KGC engines}, based on open data from the transport domain. We create Madrid-GTFS-Bench, a comprehensive benchmark for virtual knowledge graph construction, which considers multiple data formats and different data scales. Several engines are evaluated with a single benchmark so as to assess the current status of virtual knowledge graph access. Additionally, all code, datasets and queries are available, following Open Science principles, to facilitate reproducibility and further extensions.
\end{enumerate}

Chapter \ref{chapter:mappig-translation} describes the mapping translation concept and its property, while Chapter \ref{chapter:virtual} provides the solutions to enhance virtual knowledge graph access over heterogeneous data exploiting the aforementioned concept. Chapter \ref{chapter:benchmark} describes the GTFS-Madrid-Bench, a virtual knowledge graph construction benchmark in the transport domain, aligned with the second objective. Finally, Chapter \ref{chapter:construction} presents two approaches for scaling up the construction of materialized KGs together with evaluation methods for testing mapping compliance and performance during this process.

\section{Assumptions}
Our work is based on the following set of assumptions:
\begin{enumerate}[label=\textbf{A{\arabic*}}]
    \item Mapping rules and metadata descriptions are declarative and follow W3C standards (or their extensions). 
    \item The target schema (ontology) for integrating the source data is available, and is available in OWL.
    \item Mapping rules and metadata descriptions are available.
    \item Data are represented in formats that are not RDF.
    \item We do not have to consider data protection nor access restrictions.
    \item The size of datasets is XXXX.
    \item We are considering static datasets and not streams or APIs.
\end{enumerate}

\section{Hypothesis}

\begin{enumerate}[label=\textbf{H{\arabic*}}]
    \item It is possible to translate declarative mappings
    \item Virtualized Knowledge Graphs are improved in terms of completeness and performance over raw data exploiting declarative mapping rules and metadata annotations.
    \item Not only size data is relevant in the evaluation of KG construction engines
    \item A benchmark blablabla is able to stress and provide a full overview of the state of different engines 
\end{enumerate}

\section{Restrictions}

\begin{enumerate}[label=\textbf{R{\arabic*}}]
    \item Data must be located in the same physical place as XXXXX.
    \item We only consider declarative mapping rules following the RML+FnO~\citep{de2017declarative} specification and metadata annotations in CSVW for Morph-CSV and R2RML for Morph-GraphQL.
    \item The source data generated at scale for evaluating KG construction systems do not need cleaning or preparation functions.
\end{enumerate}
