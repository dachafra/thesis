\chapter{State of the Art}
\label{chap:soa}

In this chapter, we introduce the current state of the art in knowledge graph construction using declarative mapping rules. We provide an overview of approaches, techniques and methodologies for constructing and querying (virtual) knowledge graphs based on semantic web technologies. We describe the declarative annotations and mapping languages specifications that have been proposed to construct these kind of data models together with their main benefits. Finally, we present the current methodologies to evaluate the quality of knowledge graph construction engines such as benchmarkings and test-cases.


\section{Data Integration and OBDA}
A data integration system (DIS) is defined as the process of making available a set of different sources through a common view~\citep{Lenzerini02}. Formally, it is described as $DIS$ = $\langle \mathcal{G}, \mathcal{S}, \mathcal{M} \rangle$, where:
\begin{itemize}
    \item $\mathcal{G}$ is the \textit{global schema}, expressed in a language $\mathcal{L_G}$ over an alphabet $\mathcal{A_G}$.
    
    \item $\mathcal{S}$ is the \textit{source schema}, expressed in a language $\mathcal{L_S}$ over an alphabet $\mathcal{A_S}$.
    
    \item $\mathcal{M}$ is the \textit{mapping} between $\mathcal{G}$ and $\mathcal{S}$, constituted by a set of assertions matching queries over $\mathcal{S}$ and $\mathcal{G}$, in order to establish correspondences between concepts in both schemes.
\end{itemize}

Different types of data integration systems have been proposed. Data warehouses~\cite{vassiliadis2009survey} are proposed to integrate multiple and heterogeneous data sources in a centralized place, which is usually known in enterprises ecosystems as an \textit{extract-transform-load} process (ETL). In the semantic web context, the ETL concept is equivalent to a materialized KG construction process. In contrast, mediators~\citep{wiederhold1992mediators} have been proposed as another data integration approach where the data remains in the data sources. For accessing the data, queries defined over the global schema are translated to the source schema and executed. There are multiple systems that implement these ideas with their corresponding optimizations~\citep{tsimmis1994,rajaraman1996querying,roth1997don}. In semantic web, this process is known as virtual KG construction. 

Ontology based data access (OBDA) and integration (OBDI)x are a kind of data integration systems where the global schema is defined by an ontology~\cite{poggi2008linking}. The formal framework presented in~\citep{xiao2018obdasurvey} defines an OBDA specification as a tuple $P$ = $\langle O,S,M\rangle$ where $O$ is an ontology, $S$ is the source schema, and $M$ a set of mappings. Additionally, an OBDA instance is defined as a tuple $PI$ = $\langle P,D\rangle$ where P is an OBDA specification and $D$ is a data instance conforming to $S$. 

\section{Representation and Query Language for the Semantic Web}

\subsection{RDF: Resource Description Framework}

\subsection{SPARQL}




\section{Annotations in Knowledge Graph Construction}
One of the main components for the construction of knowledge graphs are the annotations. Additionally to the mapping rules, that relate the target model with the input sources in a typical data integration system definition, we include in the annotations set the constraints concept. In a DIS, the constrains property allows to: i) define ad-hoc transformation functions that permit the cleaning and preparation of the input data; ii) definition of metadata annotations to describe the content of the input source. This property is essential during a knowledge graph construction process as it is able to deal with the typical features of heterogeneous data sources such as the absence of a well-defined and fixed data schema, a normalized database instance or the non-explicit declarations of relations among the sources. We start this section discussing existing approaches for the design of mappings. Then, we describe the current mapping language specifications and their standardization through the W3C. Finally, we present approaches to define, declaratively, constraints over a DIS.

\subsection{Mapping Rules}
The mapping layer contains information about how the input sources are related with the target model. There are two basic approaches for defining mapping rules in a data integration system: Local as a View (LAV) and Global as a View (GAV). In semantic web, the usual followed approach to define this rules is the Global as View one. We now provide a description of each proposals more in detail.

\subsubsection{Local as a View Mapping rules (LAV)}
In \citep{ullman1997information} the elements of the source schema $S$ are mapped  to a query $Q_G$ over the target schema $G$. The main benefits of this approach is that it supports continuous changes of the source schema (e.g., adding new sources or modify their underlying representation) since there is no need to change the query processing component. Thus, LAV is usually useful when the global schema $G$ is stable but the local schema $S$ may suffer modifications over the time. However, one of its main disadvantages is that cannot represent source $S$ information if it is not modeled in the global schema, hence, the approach usually provides partial answers for a query $Q_G$. Query translation following this approach is not a trivial process, as the $Q_G$ has to be translated to an equivalent query over the source schema $S$. These techniques are usually known as query translation using views~\citep{halevy2001answering}.

ToDo: ADD EXAMPLE

\subsubsection{Global as a View Mapping Rules (GAV)}
Global as view are defined in \citep{halevy2001answering}, where each element of the global schema $G$ is mapped to a query over $Q_S$ the source schema $S$. Opposed to LAV approach, the benefits of following a GAV approach is that it supports changes over the global schema $G$, as the queries are defined following the source schema $S$. Although there are no theoretical limitations to provide access to other data formats, ontology-based data integration processes have been traditionally focused on allowing the integration of relational databases as source schema, based on SPARQL-to-SQL translation techniques. Due to the aforementioned limitations in these techniques for LAV approaches, most of the semantic web mapping rules specifications following the GAV approach (e.g., R$_2$O, DR2Q, R2RML). 

ToDO: ADD EXAMPLE

\subsubsection{R2RML: W3C Recommendation}
Since 2012, R2RML 

\subsubsection{RML: Extending R2RML for Heterogeneous Data}

\subsection{Declarative Constraints: Transformation Functions and Metadata}
Data constraints play a key role during a data integration process~\citep{cali2002data}. This property allows to validate an input dataset $D$ against an schema $S$. In previous proposals of knowledge graph construction over relational database (OBDA), they assume the existence of integrity constraints explicitly defined over the schema $S$ to propose their optimizations in SPARQL-to-SQL processes. Additionally, mapping recommendations (i.e., R2RML) for RDB2RDF approaches declare that cleaning or preparation steps are not part of the KG construction process and they have to be performed before running it. However, during the construction of a KG from heterogeneous data sources, these data may not be normalized, and information about relationships or column/key names are not always descriptive or homogeneous, among other possible issues. Hence, data consumers are usually forced to apply ad-hoc or manual data wrangling processes to consume these kind of data. In order to try to avoid manual and not reproducible cleaning/preparation steps, there are a set of declarative proposals to allow the description of constraints over data on the web. Specifically, we will be focused on two different ways to declare constraints: extensions of mapping specifications for include the possibility to define transformation functions inside the mapping rules and metadata to describe data content on the web. 

\subsubsection{Transformation Functions in Mappings}
Rahm and Do~\citep{rahm2000data} have reported the relevance of data transformations expressed with functions during data curation and integration. Grounding on this statement, different approaches have been proposed for facilitating the definition of functions to enhance data curation (e.g., \citep{galhardas2001declarative,GuptaSKGTM12,raman2001potter}). Similarly, declarative languages have been proposed to allow for the definition of functions in the mappings. An approach independent of a specific implementation context is described in~\citep{demeester2019implementation}. It enables the description, publication and exploration of functions and instantiation of associated implementations. The proposed model is the so called Function Ontology~\citep{de2016ontology} and the publication method follows the Linked Data principles. It is used as an extension over RML mapping rules to declarative allow the declaration of these transformation functions~\citep{de2017declarative}. Previous works related to this topic focus on developing ad-hoc and programmed functions. For example, R2RML-F~\citep{debruyne2016r2rml} and FunUL~\cite{junior2016funul,junior2016incorporating} allow using functions in the value of the \texttt{rr:objectMap} property, so as to modify the value of the table columns from a relational database first (R2RML-F) and other kinds of formats after (FunUL). KR2RML~\citep{slepicka2015kr2rml}, used in Karma, extends R2RML by adding transformation functions in order to deal with nested values. OpenRefine enables such transformations with the usage of GREL functions, which can be used in its RDF extension. 

ToDo: add FnO examples

\subsubsection{Metadata for data on the web}


\section{Knowledge Graph Construction Engines}

\section{Evaluation of Knowledge Graph Construction}

\subsection{Test-Cases}
\subsection{Benchmarks}



\section{Conclusions}
