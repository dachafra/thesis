\section{Virtual Knowledge Graph Construction over Tabular Data}
\label{chap6_morphgcsv}
Albeit extensively used, tabular representations impose various data management challenges to advanced users (e.g., developers, data scientists). The lack of a unified way to query tabular data, something available in other formats (e.g., RDB, JSON, XML), hinders the integration of data sources, especially those having datatype inconsistencies. Moreover, data may not be normalized, and information about relationships or column names are not always descriptive or homogeneous. Hence, data consumers are usually forced to apply ad-hoc or manual data wrangling processes to consume such data. 

Traditional virtual KGC approaches, usually, rely on loading tabular data into SQL-based systems\footnote{\url{https://github.com/oeg-upm/morph-rdb/wiki/Usage\#csv-files}}$^,$\footnote{\url{https://ontop-vkg.org/tutorial/mapping/primary-keys.html}}(e.g., MySQL, Apache Drill, Spark SQL, Presto) to perform well-known query translation techniques. However, the correctness and optimization of these techniques are supported by the main assumption about the existence of constraints over the source data (i.e., a good physical design of the relational database instance). Their absence during a virtual KGC process over tabular data directly impacts completeness and performance of these techniques and hence may affect negatively the results. Completeness is affected because of heterogeneity issues in data sources (e.g., datatype CSV columns are simply treated as string-type SQL columns). Performance is impacted because indexes are not created based on basic relational constraints, i.e., primary and foreign key constraints are not defined in the schema. Consequently, query translation optimization techniques that commonly exploit indexes (e.g., ~\citep{rodriguez2015efficient,priyatna2014formalisation}) may not produce the expected results whenever the constraints have not been applied, or the indexes have not been created.

KGC annotations such as the W3C recommendation to annotate tabular data, CSVW~\citep{tennison2015model} and some extensions of standard mapping rules (e.g., RML+FnO~\citep{de2017declarative}) are commonly used to describe constraints over a tabular dataset. For example, we can indicate the format, define integrity constraints, or declare data types. The majority of virtual KGC engines~\citep{priyatna2014formalisation,endris2019ontario} do not include this information. Those engines that have partially included these constraints (e.g., Squerall~\citep{mami2019squerall} parses RML+FnO mapping rules) are not fully documented; i.e., there is no explanation of how these constraints are taken into account. The definition of a workflow that includes the exploitation of these tabular annotations during a virtual KGC process will ensure correct and optimized SPARQL-to-SQL translations.

We address the limitations of current SPARQL-to-SQL query translation techniques over tabular data, which enforce and demand unreproducible and hard manual work for the application of constraints to ensure efficient query processing and query completeness\footnote{\url{https://github.com/oeg-upm/morph-rdb/wiki/Usage\#csv-files}}$^,$\footnote{\url{https://ontop-vkg.org/tutorial/mapping/primary-keys.html}}$^,$\footnote{\url{https://ontop-vkg.org/tutorial/mapping/foreign-keys.html}}. Our goals are to (i) define a framework that includes the application of a set of constraints over tabular data, and (ii) define a set of efficient operators that apply each type of constraint to improve query completeness and performance (e.g., removal of duplicates, normalization of input sources or application of transformation functions). 

We propose a set of new steps to be aligned with the current KGC workflow. Further, we implement Morph-CSV, and evaluate its behavior embedded on top of two well known open source SPARQL-to-SQL engines, in comparison with previous approaches. Our main contributions, extending the contribution C1.2 defined in Chapter \ref{chap:objectives}, are as follows:
\begin{enumerate}
\item C1.2.1: Definition of the concept of Virtual Tabular Dataset (VTD) composed by a tabular dataset and its corresponding annotations, as well as its alignment with the current definition and assumptions of the OBDA framework~\citep{xiao2018obdasurvey}.
\item C1.2.2: Morph-CSV, a framework that implements a constraint-based KGC workflow for tabular datasets; it receives a VTD and a SPARQL query as inputs and outputs an OBDA instance. Morph-CSV performs the following steps: (i) generation of the constraints based on information on the VTD; (ii) selection of sources and attributes needed to answer the query; (iii) pre-processing of the selected sources applying some of the constraints; and (iv) physical implementation of the corresponding RDB instance and associated schema, ensuring effectiveness of the SPARQL-to-SQL translations and optimizations. Morph-CSV is engine agnostic, i.e., it can be embedded on top of any SPARQL-to-SQL engine.
\end{enumerate}

\subsection{Motivating Example}
\begin{figure}[th]
    \centering
    \includegraphics[width=0.6\linewidth]{figures/example.pdf}
    \caption[Morph-CSV motivating example]{\textbf{Motivating Example.} SPARQL query evaluation over two tabular data files in the transport domain through a common virtual KGC approach. It loads the files as single tables in an SQL-based system and uses the mapping rules for query translation. The number of results differs with respect to the expected results due the heterogeneity of the raw data. Additionally, query performance may be affected by the join condition between the two tables, the absence of indexes and the loading of columns that are not needed to answer the input query (wheelchair).}
    \label{fig:example}
\end{figure}
Since May 2017, the publication of a new directive by the EU Commission on discoverability and access to public transport data across Europe\footnote{\url{https://ec.europa.eu/transport/themes/its/road/action_plan/nap}} has motivated the development of solutions for multi-modal travel information services. This document states that transport data should be available through national access points (NAP), e.g., databases, data warehouses, and repositories. Consider the \emph{de-facto standard} for publishing open data in the transport domain, GTFS\footnote{\url{https://developers.google.com/transit/gtfs/reference/}} which has already discussed in Section \ref{chapter5:sec-bench}. Linking these feeds based on their stops enables route planners to offer multi-modal routes, a route that can be created using various transportation types.
Albeit straightforward and simple to use, GTFS feeds do not allow for the definition of integrity constraints such as primary or foreign keys (i.e., permanent and unique identifiers for such entities). As a consequence, data integrity cannot always be guaranteed. 

Consider the GTFS feeds from the metro and buses of Madrid's city (used in the GTFS-Madrid-Bench, Section \ref{chapter5:sec-bench}); these two feeds have several stops and stations in common. Different transport authorities create them, and the names of their stops are defined in various manners. Although these types of entities can be represented, the unique identification and relationships among them cannot be explicitly expressed. Figure~\ref{fig:example} depicts a portion of these two GTFS feeds. As it is usual in open datasets, stop names do not follow a standard structure (e.g., ``Colonia Jardin'' in \textit{bus\_stops.csv} and ``Colonia\_jardin'' in \textit{metro\_stops.csv}). A similar issue is present in closing dates, where there are multi-valued cells, and their format is not the standard one (e.g., yyyy-MM-dd). Suppose a user is interested in collecting information about bus and metro stops with the same name and information related to their closing dates during holidays; Figure~\ref{fig:example} presents the SPARQL query describing this request. Following the approach commonly employed by typical KGC engines, the two files would be loaded into an SQL-system and treated as single tables. The obtained result set only contains one answer where the stop names in the two data sources are identical (``Noviciado''). However, the expected result set should include more answers by joining among the bus and metro's stop names through the normalization of multi-valued date columns. 

Query's performance may also be affected whenever a join condition is executed between the stop names of both files. Furthermore, the absence of possible indexes in these joining columns makes ineffective the typical optimizations applied in a SPARQL-to-SQL process. Nonetheless, to effectively exploit the indexes to scale-up the execution of the translated queries, the satisfaction of the unique and foreign integrity constraints should be ensured. The manual and ad-hoc definition of the relational schema representing these tables and the corresponding integrity constraints will overcome this problem. Nevertheless, this task is time-consuming, and reproducibility is not ensured. In this section, we propose Morph-CSV, a constraint-based KGC framework capable of exploiting standard tabular data annotations (e.g., RML or CSVW) to generate the required constraints ensure the integrity of the tabular schema in terms of unique identifiers and foreign keys. Moreover, Morph-CSV applies metadata annotation from CSVW to generate domain-specific constraints. As a result, Morph-CSV enhances query completeness and performance of SPARQL-to-SQL techniques, in compliance with OBDA assumptions.


\subsection{Virtual KGC over Tabular Data}
This section describes a set of challenges demanded be addressed whenever tabular data is queried in a virtual KGC framework. Further, we describe relevant proposals for annotating tabular datasets and their alignment with the identified challenges.

\subsubsection{Querying challenges under virtual KGC for tabular data}
There are specific challenges on querying tabular datasets using an virtual KGC approach that have not been tackled by existing techniques. We will describe those challenges and explain how they may have a negative effect in terms of completeness and performance of query-translation approaches:
\begin{itemize}
    \item \textbf{Updated results:} Existing frameworks load all of the tabular input files that are specified as sources in the mapping rules into a SQL database before executing the query-translation process. This step has to be repeated whenever a SPARQL query is evaluated to ensure up-to-date results, resulting in unnecessary longer loading time, affecting, thus, the performance.
    \item \textbf{Normalization:} Tabular data formats do not provide restrictions on how to structure data. As a result, cells may contain multiple values, and one file may represent multiple entities. Having non-normalized tables may affect the completeness of the query. When a tabular source with multiple-valued cells is loaded into an RDB table, the cell's value is interpreted by the RDBMS as an atomic value, reducing, thus, completeness for queries that filter or ``join'' on the corresponding column. Representing several entities in a single file may lead to duplicate answers, and in turn, decrease query answering performance.
    \item\textbf{Heterogeneity:} Tabular data normally contain values that need to be transformed before query evaluation (e.g., column default values or normalization of date formats). Since there may be different formats for the same datatype or default values that may have not been included in the dataset, query completeness can be affected.
    \item \textbf{Lightweight Schema:} Most of the tabular data only provide minimal information about their underlying schema in the form of column names in the header, if at all present. Also, although there is implicit information on keys and relationships among sources, there is no way to specify primary key or foreign key constraints. The same can be said about indexes and datatypes. The existence of this type of information is assumed~\citep{xiao2018obdasurvey} in an virtual KGC approach for performing optimizations in query evaluation techniques. Therefore, the lack of this information affects the performance of these engines.
\end{itemize}

Although some of the aforementioned challenges are not only specific to tabular datasets and are proposed in several data integration approaches~\citep{golshan2017data,halevy2006data,doan2012principles} there are two main reasons why it is important to address these problems in this context: first, the number of tabular datasets available in the web of data is enormous and still growing and these challenges were not taken into account in previous virtual KGC proposals; second, although there are declarative proposals to handle these issues in the state of the art like CSV on the Web~\citep{tennison2015model} for metadata annotations, or mapping languages that include transformation functions to deal with heterogeneity (e.g., RML+FnO~\citep{de2017declarative} or R2RML-F~\citep{debruyne2016r2rml}), there is not yet a proposal that exploits the information from these inputs including their application in the form of constraints into a common virtual KGC workflow.



% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
\begin{table*}[t]
\centering
\caption[Relevant properties of CSVW and RML+FnO]{Properties of CSVW and RML+FnO that can be used to address the challenges of dealing with tabular data in a virtual KGC approach}
\label{tab:features}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{c|l|l}
\hline
\rowcolor{orange!50} 
\textbf{General Challenge} & \multicolumn{1}{c|}{\textbf{Detailed Challenges}} & \multicolumn{1}{c}{\textbf{Relevant Properties}} \\ \hline
 Updated results & Select relevant sources and columns &  SPARQL + RML+FnO\\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}c@{}}Lightweight\\ Schema\end{tabular}} & Describe the corresponding concept & rr:class \\ \cline{2-3} 
 & Describe the corresponding property & rr:predicateMap \\ \cline{2-3} 
 & Specify NOT NULL constraint & csvw:required \\ \cline{2-3} 
 & Column datatype & csvw:datatype \\ \hline
\multirow{7}{*}{Heterogeneity} & Domain values & csvw:minimum, csvw:maximum \\ \cline{2-3} 
 & Specify the format of a column & csvw:format \\ \cline{2-3} 
 & Transform value & fnml:functionValue \\ \cline{2-3} 
 & Default for missing values & csvw:default \\ \cline{2-3} 
 & Specify NULL values & csvw:null \\ \cline{2-3} 
 & Add header to a CSV file & csvw:rowTitles \\ \hline
\multirow{4}{*}{Normalization} & Primary Key & csvw:primaryKey \\ \cline{2-3} 
 & Foreign Key & csvw:foreignKey \\ \cline{2-3} 
 & Relationships between columns & rr:parentTriplesMap + rr:joinCondition \\ \cline{2-3} 
 & Mutiple entities in one source & rr:TriplesMap + rml:logicalSource \\ \cline{2-3} 
 & Support for multiple values in one cell & csvw:separator \\ \cline{2-3} 
 \hline
\end{tabular}%
}
\end{table*}

\subsubsection{KGC annotations for tabular data}
In Table \ref{tab:features}, we summarize the relevant properties from RML+FnO and CSVW that can be used to address the  challenges identified in the previous section. Additionally, we provide a detailed description of these properties:
\begin{itemize}
    \item \textbf{Metadata.} The property \texttt{csvw:rowTitles} can be used to specify column names in case the first row is not used to specify them.
    
    \item \textbf{Transformation functions.} String concatenation functions are supported by both CSVW (\texttt{csvw:aboutUrl}, \texttt{csvw:valueUrl}) and the RML property (\texttt{rr:template}). In addition, more complex functions can be declaratively specified using RML+FnO, specifically, with the \texttt{fnml:functionValue} property. Finally, two special cases of transformation functions in the context of OBDA are related to how default values and NULL representations have to be generated in the RDB instance. These two cases can be handled by CSVW properties: \texttt{csvw:defaultValue} and \texttt{cvwv:null}.
    
    \item \textbf{Domain Constraints.} CSVW allows for the specification of the datatype (\texttt{csvw:datatype} property) and format (\texttt{csvw:format} property) of tabular columns. CSVW also provides a couple of properties (e.g., \texttt{csvw:mininum} or \texttt{csvw:maximum}) to specify the range of numerical columns and a property \texttt{csvw:required} to specify the NOT NULL constraint over the column of a table.
    
    \item \textbf{Integrity Constraints.} In CSVW the property \texttt{csvw:primaryKey} can be used to declare explicitly the primary key of a table. As for the foreign key, the use of RML's properties \texttt{rr:parentTriplesMap} together with the property \texttt{rr:joinCondition} can be seen as an indication that the parent column used over this rule could be a foreign key, or at least that a relation exists. CSVW provides an explicit way to declare whether a column is a foreign key, using the \texttt{csvw:foreignKeys} property. 
    
    \item \textbf{Normalization.} The property \texttt{csvw:separator} from CSVW indicates the character used to separate multiple values in the cells of a CSV column, which is relevant when a CSV file is in 1NF. Multiple RML TriplesMap using the same data source can be used as an indication that the source contains multiple concepts (2NF).
\end{itemize}



\subsection{The Morph-CSV Framework}

The formal framework presented in~\citep{xiao2018obdasurvey} defines an OBDA\footnote{In this section we use OBDA as synonym of virtual KGC} specification as a tuple $P$ = $\langle O,S,M\rangle$ where $O$ is an ontology, $S$ is the source schema, and $M$ a set of mappings. Additionally, an OBDA instance is defined as a tuple $PI$ = $\langle P,D\rangle$ where P is an OBDA specification and $D$ is a data instance conforming to $S$. In a virtual OBDA framework, queries are posed over a conceptual layer and then translated to queries over the data layer using information in the mappings. There is a set of assumptions over the framework that support the possibility of doing query translation and  ensuring  semantic preservation in the process, together with the application of  optimization techniques proposed in the state of the art. To motivate our proposal, we have to establish what are the main assumptions made in previous proposals and their impact when data is represented in tabular form.


\begin{figure}[th]
  \centering
  \subfloat[Baseline approach]{
    \includegraphics[width=0.48\linewidth]{figures/naive-approach.pdf}  
    \label{fig:naive}
  }
  \subfloat[Enhanced virtual OBDA workflow.]{
  \includegraphics[width=0.48\linewidth]{figures/vtd-approach.pdf} 
  \label{fig:vtd}
  }
\caption[Virtual OBDA approaches for tabular data]{\textbf{Virtual OBDA approaches for tabular data.} The baseline approach creates the schema and relational database instance extracting file and columns names from the tabular dataset. The proposed workflow exploits the information from the mapping rules and metadata to extracted a set of constraints and applying them over the tabular data to generate the schema and the relational database instance.}
\label{fig:obda}
\end{figure}


\subsubsection{OBDA assumptions}
Analyzing the definition of OBDA in~\citep{xiao2018obdasurvey} and its extension for NoSQL databases defined in~\citep{botoeva2019ontology} we identified a set of assumptions made over the framework and their impact when the dataset is tabular:
\begin{itemize}
    \item There is a native query language $QL$ for $D$. For a tabular dataset, there is no native query language for querying this format, which generates an important difference with other common formats for exposing raw data on the web such as JSON and XML as they include methods to query them (JSONPath, XPath). This is the main issue that needs to be solved in order to query tabular datasets in a virtual OBDA context and has a direct impact on the rest of the assumptions.%, that have been solved in a naive manner.
    \item $S$ typically includes a set of domain and integrity constraints. In the case of querying a tabular dataset $D_{tabular}$, $S$ is defined using column names extracted from $D_{tabular}$ and it does not include any  constraint types (neither domain nor integrity constraints). This has a negative impact not only in terms of query execution time but also over query result completeness as there will be queries that cannot be executed due to the lack of explicit domain constraints.
    \item $D$ is an RDB instance or  a NoSQL database instance, %equipped with 
    that includes an RDB wrapper able to provide a relational view over $S$ and $D$. In the context of a tabular dataset $D_{tabular}$, $D$=$R_{wrapper}(D_{tabular})$ where $R_{wrapper}$ is a relational database wrapper that satisfies $S$.
\end{itemize}

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figures/architecture.pdf}
    \caption[The Morph-CSV Framework.]{\textbf{The Morph-CSV Framework.} Morph-CSV extends the starting phase of a typical OBDA system including a set of steps for dealing with the identified tabular data querying challenges. The framework first, extracts the constraints from mappings and tabular metadata and then, implements them in a set of operators that are run before executing the query translation and query execution phases, which can be delegated to any SPARQL-to-SQL engine. The mapping rules are translated accordingly to the modified tabular dataset to allow its access by the underlying OBDA engine.}
    \label{fig:workflow}
\end{figure}
\subsubsection{From a virtual tabular dataset to an OBDA instance}
Based on the previous OBDA assumptions, we define the concepts and functions to address the problem of querying a tabular dataset in OBDA.
\begin{definition}
\label{def:vtd}
A virtual tabular dataset is defined as a tuple $VTD$=$\langle D_{tabular},O,M,MD\rangle$ where $D_{tabular}$ is a tabular dataset that is composed of a set of data sources, defined as $\mathcal{D}_{tabular}$ = $\{s_1,\ldots, s_n\}$ and where each $s_i$ is a tabular relation defined over the domains of the attributes $Att(s_i)=\{A_{i1},\ldots,A_{im}\}$\footnote{A relation is defined as the subset of the Cartesian product of the domains of the attributes.}, where $m$ is the number of attributes of $s_i$. $O$ is an ontology, and $M$ is a set of global as view mappings between $O$ and $schema(D_{tabular})$\footnote{The set of the attributes of each tabular relation in $D_{tabular}$, i.e., $schema(D_{tabular})=\{Att(s_i),\ldots,Att(s_n)\}$}. $MD$ is a set of metadata tabular (domain) annotations, where for each $s_i$ there exists a set $\{(A_{i1},Type(A_{i1})),\ldots,(A_{im},Type(A_{im}))\}$ in $MD$. 
\end{definition}

\textit{Example 1.} The virtual tabular dataset of the GTFS of Madrid's metro system can be defined as $VGTFS^{metro}_{madrid}$ where the dataset is composed by of 10 different tabular sources in CSV format $GTFS_{tabular}$, $LinkedGTFS$\footnote{\url{https://lov.linkeddata.es/dataset/lov/vocabs/gtfs}} is the ontology, the mappings $RML+FnO_{GTFS}$, following the RML+FnO~\citep{de2017declarative} specification, define the relation between the input sources and the ontology and, finally, the metadata $CSVW_{GTFS}$ is defined according to the W3C recommendation, CSVW~\citep{tennison2015model}, specifying a set of constraints extracted from the GTFS reference data model\footnote{\url{https://developers.google.com/transit/gtfs/reference}}.


Given a $VTD$, we define the function $\theta(VTD)=PI$ where $PI$ is an OBDA instance $PI=\langle P,D\rangle$ where $D$=$R_{wrapper}(D_{tabular})$ and $P=\langle O,S,M\rangle$ is an OBDA definition where $S$ does not contain any type of constraint.
We extend the function $\theta(VTD)$ with the aim of enhancing the virtual OBDA baseline approach over tabular data. We define $\theta^{++}(VTD)$=$PI$ as a function that extracts a set of constraints from $M$ and $MD$ and then applies them over $D_{tabular}$ to obtain $PI$. More in detail, the function can be expressed as $\theta^{++}(VTD)$=$\gamma(D_{tabular},O,M,\psi(M,MD))$ where the function $\psi(M,MD)= C$ extracts a set of constraints from OBDA annotations for tabular data. Then, $\gamma(D_{tabular},O,M,C)$ applies the constraints $C$ over $D_{tabular}$ to create a relational database schema $S^{'}$ and its corresponding instance $D^{'}$. In summary, the final output is an OBDA instance $PI^{'}=\langle P^{'},D^{'}\rangle$, where $D^{'}$ is a relational database instance that is compliant with the main assumptions of the OBDA framework and $P^{'}=\langle O,S^{'},M^{'}\rangle$ where $S^{'}$ contains a set of domain and integrity constraints and $M^{'}$ are the mapping rules that define the relations between $O$ and $S^{'}$. Following the proposed workflow in Figure \ref{fig:vtd}, the user first defines the query based on the concepts defined in the ontology, and then, during the starting phase, the $\theta^{++}(VTD)$ is performed. During the execution of the function, first, the constraints from mappings and annotations ($\psi(M,MD)$) are extracted, and then the OBDA instance $PI^{'}$ is generated where the constraints are applied to efficiently create the schema $S^{'}$ and the relational database instance $D^{'}$. Mapping rules are also translated, from $M$ to $M{'}$ to be aligned with the new created schema.
\begin{table}[t]
\centering
\caption[Functions and Annotations applied by Morph-CSV]{Summary of constraints, corresponding functions and OBDA annotations applied by Morph-CSV}
\label{tab:summary}
\resizebox{0.9\textwidth}{!}{%
\begin{tabular}{l|l|l|l|l}
\hline
\rowcolor{orange!50}
\multicolumn{1}{c|}{\textbf{Step}} & \multicolumn{1}{c|}{\textbf{Constraint/Improvement}} & \multicolumn{1}{c|}{\textbf{Rule/Annotation}} & \multicolumn{1}{c|}{\textbf{Function}} & \multicolumn{1}{c}{\textbf{Challenge}} \\ \hline
\multirow{2}{*}{Extraction} & \multirow{2}{*}{Reduce search space} & SSG from Query & select\_annotations & \multirow{2}{*}{Selection} \\ \cline{3-4}
 &  & Mapping Rules & select\_sources &  \\ \hline
\multirow{2}{*}{\begin{tabular}[c]{@{}l@{}}Data \\ Normalization\end{tabular}} & 2NF & csvw:separator & split & \multicolumn{1}{c}{\multirow{2}{*}{Normalization}} \\ \cline{2-4}
 & 3NF & \begin{tabular}[c]{@{}l@{}}TriplesMap with\\ same source\end{tabular} & cut & \multicolumn{1}{c}{} \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Data \\ Preparation\end{tabular}} & \multirow{2}{*}{Standarization} & \begin{tabular}[c]{@{}l@{}}csvw:null, csvw:default\\ csvw:format, etc.\end{tabular} & sub & \multicolumn{1}{c}{\multirow{3}{*}{Heterogeneity}} \\ \cline{3-4}
 &  & fnml:functionValue & create & \multicolumn{1}{c}{} \\ \cline{2-4}
 & Duplicates & - & duplicates & \multicolumn{1}{c}{} \\ \hline
\multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Schema \\ Creation and\\ Load\end{tabular}} & Primary Key & csvw:primaryKey & primaryKey & \multirow{4}{*}{\begin{tabular}[c]{@{}l@{}}Lightweight \\ Schema\end{tabular}} \\ \cline{2-4}
 & Foreign Key & csvw:foreignKey & foreignKey &  \\ \cline{2-4}
 & DataType & csvw:datatype & datatype &  \\ \cline{2-4}
 & Index & \begin{tabular}[c]{@{}l@{}}selectivity on mapping \\ join conditions\end{tabular} & index &  \\ \hline
\end{tabular}%
}
\end{table}
\textit{Example 2.} The process of applying the function $\theta^{++}(VGTFS^{metro}_{madrid})$ generates the OBDA instance $PIGTFS^{metro}_{madrid}$. The features of this output are a relational database schema $GTFS_{schema}$, a relational database instance $GTFS_{SQL}$ compliant with the defined schema, and a set of mapping rules following the R2RML W3C recommendation, $R2RML_{GTFS}$, that represent the relations between $GTFS_{schema}$ and the $LinkedGTFS$ ontology. 


Constraints are conjunctive rules specified for tabular data that restrict the valid data in one or more tables. $C$ is a set of constraints, where each constraint $c$ is a logical statement that expresses the condition that needs to be satisfied by the data in order to be valid. Each constraint is applied through a function.

\textit{Example 3.} CSVW allows expressing a primary key constraint for a table. The function $\psi(M,MD)=C$ generates the corresponding constraints in the form of a function $primaryKey(t,a)$ that applies this constraint to a source $t$ and a set of columns $a$, and generates a primary key in the output schema. 

Given an OBDA instance $PI$=$\langle\mathcal{P,D}\rangle$, we define the function $eval(Q,PI)$, that retrieves a SPARQL answer set that is the result of the translation of $Q$ from SPARQL to SQL using the mapping rules $M$ defined in $P$, and then evaluating the query directly over $D$. 


\subsubsection{Problem statement and solution}
Based on the preliminaries and assumptions on the OBDA framework, we now define the problem that we address in this paper and Morph-CSV, our proposed solution.

\paragraph{}\noindent\textbf{Problem statement:} Given a $VTD$, the problem of OBDA query translation over tabular data is defined as the problem of explicitly enforcing implicit constraints $C$ extracted from mapping rules $M$ and metadata $MD$ on a tabular dataset $D_{tabular}$, such that: 
\begin{itemize}
    \item The number of results obtained in the evaluation of the SPARQL query $Q$ over the function $eval(Q,\theta^{++}(VTD))$ is equal or greater than the number of results in the evaluation of the same query $Q$ over the function $eval(Q,\theta(VTD))$, i.e., \\ $\#answers(eval(Q,\theta^{++}(VTD))) \geq  \#answers(eval(Q,\theta(VTD)))$.
    \item The total execution time of evaluating a SPARQL query $Q$ over $eval(Q,\theta^{++}(VTD))$ is %decreased compared to the evaluation of the 
    less than or equal than the total execution time of the same SPARQL query $Q$ over the function $eval(Q,\theta(VTD))$, i.e.,\\ $time(eval(Q,\theta^{++}(VTD))) \leq 
    time(eval(Q,\theta(VTD)))$. 
\end{itemize}


\noindent\textbf{Proposed solution:} We propose Morph-CSV, an alternative to the traditional OBDA workflow for query translation when the input is a tabular dataset (see Figure \ref{fig:vtd}). Morph-CSV relies on the function $eval(Q,\theta^{++}(VTD,\psi(M,MD)))$, to apply the tabular dataset constrains. Thus, Morph-CSV extends a typical OBDA workflow by including a set of steps for a maintainable extraction and efficient application of constraints. The workflow proposal is as follows:
\begin{itemize}
    \item \textbf{Constraint Extraction}: the evaluation of the function $\psi(M,MD)$ produces as output the set of constraints $C$; it exploits the information defined in the annotations of $M$ and $MD$, i.e., the set of metadata tabular annotations and mapping rules, respectively. At implementation level they are expressed as CSVW specifications and RML+FnO mapping rules. 
    \item \textbf{Source Selection}: in this step the sources required to evaluate the SPARQL query $Q$ are selected. The required data sources correspond to the set of sources in the result of unfolding~\citep{poggi2008linking} $Q$ according to the mapping rules in $M$. 
    \item \textbf{Normalization}: metadata and mapping rules are used to extract functional dependencies between the attributes of the data sources. The algorithm by Beeri et al.~\citep{Beeri1978ASI} is followed to transform tabular data sources into tabular relations that meet third normal form (3NF).  
    \item \textbf{Data Preparation}: application of the transformation functions based on the extracted domain constraints and on a set of optimization techniques that adapt the ideas proposed in~\citep{jozashoori2019mapsdi,iglesias2020sdm,jozashoori2020funmap} to a virtual OBDA environment. These techniques are explained in detail in Chapter \ref{chapter:construction}.
    \item \textbf{Schema Creation and Load}: creation of the schema and loading the data into the database instance applying a set of rules for index creation. 
    \item \textbf{Query Translation and Execution}:the evaluation of the query $Q$ is delegated to any OBDA SPARQL-to-SQL engine.
\end{itemize}
 We show the workflow of Morph-CSV in Figure \ref{fig:workflow} with the inputs and outputs of each step. 


\begin{figure}[t!]
  \centering
  \subfloat[Input SPARQL query.]{
  \includegraphics[width=0.5\linewidth]{figures/steps/selectquery.pdf}
  \label{fig:selectionq}
  } 
  \subfloat[Mapping rules selection.]{
    \includegraphics[width=0.5\linewidth]{figures/steps/selectmapping.pdf}  
    \label{fig:selectionm}
  }
\caption[Selection of mapping rules]{\textbf{Selection of Mapping Rules.} Based on the SPARQL query relevant rules are selected (in bold), the rest are discarded.}
\label{fig:selection}
\end{figure}

\subsubsection{Steps performed in the Morph-CSV framework}
We describe in detail the steps proposed in Morph-CSV together with an example extracted from the benchmark for virtual knowledge graph access, Madrid-GTFS-Bench, using the query shown in Figure \ref{fig:selectionq}, the GTFS feed from the  Madrid metro as source data, and the corresponding RML+FnO mapping rules and CSVW annotations\footnote{Resources at: \url{https://github.com/oeg-upm/gtfs-bench}}.

\subsubsection*{Constraint Extraction}
The first step performed by Morph-CSV is the extraction of the constraints that are applied to improve query execution and completeness. Morph-CSV benefits from having declarative and standard approaches to generalize this step: CSVW~\citep{tennison2015model} for the metadata; and RML+FnO~\citep{de2017declarative} for mapping rules and specific transformation functions. Thus, maintainability, understandability and readability of this process are improved in comparison with ad-hoc pre-processing approaches. 

Most of the constraints such as PK-FK relations, datatypes or NULL values are explicitly declared in the metadata of the sources. However, there are a set of implicit constraints such as the conditions for the normalization of sources and the creation of indexes, that require complex rules to extract them and that are explained in detail in the corresponding steps. The summary of the constraints, associated functions, and properties used from OBDA annotations to extract them, are shown in Table~\ref{tab:summary}.


\subsubsection*{Source selection}
The second step is to select the relevant sources to answer the input query. The baseline approach delegates this step to the RDBMS: it loads all the sources of the dataset in the RDB instance because it does not have information about which sources are going to be queried. This has a negative impact in the total execution time of a query. Taking the input mapping rules, Morph-CSV performs query unfolding, and pushes down source selection by executing the function $select(Q,M)$, divided into two main steps. First, Morph-CSV performs an operation to select only the relevant annotations for answering the input query, $select\_annotations(Q,M)$. It first creates the set of star shaped groups SSG$_1\ldots $SSG$_n$ of the query~\citep{vidal2010efficient} (triple patterns with the same subject)\footnote{As usual in these approaches, we assume bounded predicates in the triple patterns}. Then, for each SSG$_i$ and \texttt{rr:TriplesMap} $TM_j$ defined in $M$, the engine selects the $TM_j$ where the predicates in SSG$_i$ are contained in the set of \texttt{rr:PredicateObjectMap} (POMs) defined in $TM_j$. Finally, for each selected \texttt{rr:TriplesMap} $TM_j$, Morph-CSV only selects the POMs according to the predicates defined in the SSG$_i$, hence, removing from each $TM_j$ irrelevant rules for the input query. Using these mapping rules $M^{'}$, only relevant metadata annotations are also selected, $MD^{'}$. The obtained mapping rules in this step, $M^{'}$ and annotations $MD^{'}$, substitute the original ones in $VTD$. An example of this step is shown in Figure \ref{fig:selection}, where the input query asks for trips, their route type, routes names and corresponding time frequencies. Morph-CSV first creates the SSGs, 3 in this case, and using the predicates of each SSG, the \texttt{rr:TriplesMap} are selected from the general GTFS mapping document, discarding the rest of the rules. Then, it only selects the necessary POMs for evaluating the query such as \texttt{gtfs:startTime}, \texttt{gtfs:shortName} and \texttt{gtfs:routeType} (Figure \ref{fig:selectionm}).

Second, Morph-CSV runs $select\_sources(M)$, where it projects, from the input $D_{tabular}$, the sources and columns that are referenced in $M$, hence, relevant sources for the input query. The output of this function generates a set of new tabular sources $s_i\ldots s_n$ that substitute the original $D_{tabular}$ in $VTD$. Following the previous example, Figure \ref{fig:selection2} shows the selection of the relevant columns of  source \textit{routes.csv}, where Morph-CSV has the original source as  input (Figure \ref{fig:selection2i}), and discards the unnecessary columns of the source based on the mapping rules, obtaining as output the source with the relevant columns for evaluating the input query (Figure \ref{fig:selection2r}). Note that in this step, unnecessary sources from the input GTFS feed such as \textit{agency.csv} and \textit{stops.csv} are also discarded.

\begin{figure}[t!]
\centering
\subfloat[Original routes.csv input source.]{
    \includegraphics[width=0.75\linewidth]{figures/steps/pushdowninput.pdf} 
    \label{fig:selection2i}
}\\
\subfloat[Output of routes.csv source.]{
  \includegraphics[width=0.5\linewidth]{figures/steps/pushdownresult.pdf}  
  \label{fig:selection2r}
}
\caption[Source selection step by Morph-CSV]{\textbf{Source selection step by Morph-CSV.} Based on the selection of the rules, only route\_id and trip\_id columns are selected, discarding the rest fields.}
\label{fig:selection2}
\end{figure}

\subsubsection*{Normalization}
There are two functions for performing data normalization. The first one is the treatment of multi-values in a column. In this case, Morph-CSV performs the function $split(A_{ij},sep)$ where $A_{ij}$ is the multi-valued column of source $s_{j}$ and $sep$ is the character defined in the CSVW metadata using the \texttt{csvw:separator} property. The output is a modified $VTD$ with a new source $s_t$ containing the separated values in one column with a common identifier $ID_{ij}$ in another column and an $s_{j}^{'}$ source where the values of $A_{ij}$ are substituted by the identifier defined in $s_t$, $ID_{ij}$. Additionally, this function modifies the mapping document $M$ with a new \texttt{rr:TriplesMap} $TM_t$ generated for the new source $s_t$ and a \texttt{rr:joinCondition} between the \texttt{rr:TriplesMap} of $s_j$, $TM_j$ and $TM_t$. The application of this function is known as the normalization step for second normal form (2NF)~\citep{codd1979extending}.
 
The second function is the treatment of multiple entities in the same source. Morph-CSV takes the mapping rules and executes the function $cut(\mathcal{M},\mathcal{D}_{tabular})$. This function analyzes the mapping rules $\mathcal{M}$, and performs a 3NF~\citep{codd1979extending} normalization step over ${D}_{tabular}$ when there are two sets of mapping rules ($TM_j$ and $TM_i$) that have the same source, and the intersection of their columns in the rules only contains the join condition references. Following a similar approach as in 2NF, the output is a modified $VTD$ with a set of new sources $s_i\ldots s_n$, each one with the corresponding columns of each entity. For example, in Figure \ref{fig:normalization} we show the 3NF normalization of the \textit{routes.csv} file, that generates an auxiliary source for the \texttt{rr:TriplesMap} with the \textit{gtfs:RouteType} entity data (Figure \ref{fig:normalization}), removing that information from \textit{routes.csv}. In several data integration approaches, normalization steps are not taken into account in order to improve query execution (reducing the number of joins among sources). However, in the case of RDF, where each entity of a class has a unique URI (subject), joins cannot be reduced (see input mapping in Figure \ref{fig:selectionm}). This means that taking into account normalization steps in an OBDA context not only helps to improve query completeness, but also helps to improve performance. Additionally, normalization is also essential for allowing Morph-CSV to efficiently run data preparation steps, as we show in the next step.

\begin{figure}[t!]
\centering
\subfloat[Routes.csv after 3NF normalization step.]{
    \includegraphics[width=0.5\linewidth]{figures/steps/normalization1.pdf}  
    \label{fig:norm1}
}
\subfloat[Route\_type.csv file generated with Morph-CSV.]{
    \includegraphics[width=0.4\linewidth]{figures/steps/normalization2.pdf} 
    \label{fig:norm}
}
\caption[Normalization step by Morph-CSV]{\textbf{Normalization step by Morph-CSV.} 3NF Normalization step over the \textit{routes.csv} file generating other file with the data for \texttt{gtfs:RouteType} class.}
\label{fig:normalization}
\end{figure}

\subsubsection*{Data preparation}
In this step, Morph-CSV addresses the challenge of \textit{Heterogeneity} and executes three different functions: $duplicates$, $sub$ and $create$. First, Morph-CSV removes all duplicates in the raw data, not only the original ones, but also other duplicates that can appear during the normalization step (see Figure \ref{fig:norm}). It applies the ideas described in~\citep{jozashoori2019mapsdi}, performing $duplicates(s_{j})$ where $s_{j}$ is a source in $D_{tabular}$. As it has already been demonstrated in~\citep{jozashoori2019mapsdi,iglesias2020sdm,jozashoori2020funmap}, this step not only has a high impact on the behavior of these engines, but in this case, it also reduces the number of operations performed by Morph-CSV $sub$ and $create$, as they are defined as deterministic functions. The first one is defined as $sub(exp(A_{ij}),val)$ where $exp(A_{ij})$ is a boolean function over column $A_{ij}$ of source $s_{j}$ that when true, the value of $A_{ij}$ is substituted by $val$. There are multiple substitution functions that Morph-CSV executes such as default values, null values and date formats. This function is one of the most important for enhancing the completeness of the query (e.g., enforcing the default values of a column). The second function creates a new column in a specific source $s_{j}$. It is defined as $create(c(A_{nj},\ldots,A_{mj}))$, where $c(A_{nj},\ldots,A_{mj})$ is the application of a set of transformation functions over the columns $A_{nj},\ldots,A_{mj}$ in source $s_{j}$. This function is used to push down the application of ad-hoc transformation functions, usually defined inside the mapping rules~\citep{junior2016funul,de2017declarative}, thus, avoiding the incorporation of them inside the SQL translated query. In Figure \ref{fig:preparation} we show the \textit{route\_type.csv} file after the execution of this step. First, Morph-CSV removes the duplicates of the file obtaining as output a file with only two rows. Then, it executes the transformation function defined in the mapping rules and creates a new column in the file, generating the desired value for the subject of the class according to the LinkedGTFS ontology, ``Subway''. Additionally, the engine substitutes the definition of the transformation functions in the mapping rules by a reference to the created column. In this manner, Morph-CSV efficiently performs the $sub$ and $create$ functions directly over the raw data and together with the normalization step. Thus, the number of joins in the input query is reduced.

\begin{figure}[t!]
    \centering
    \includegraphics[width=1\linewidth]{figures/steps/creation-result.pdf}
    \caption{Data preparation of \textit{route-types.csv} file.}
    \label{fig:preparation}
\end{figure}

\subsubsection*{Schema creation and load}
The final step before translating and executing the query is the creation of an SQL schema applying the rest of the identified constraints, and loading the selected tabular data sources. Besides the typical integrity constraints that can be extracted from CSVW annotations (PK/FK), Morph-CSV implements a rule for creating indexes in the RDB instance in order to optimize the execution of query joins. In tabular datasets, it is common that the join conditions defined in the mapping rules are based on columns that are not part of PK-FK relations; thus, they are not indexed and OBDA optimizations do not have the desired effect. To address this problem, Morph-CSV gets the \texttt{rr:child} and \texttt{rr:parent} references of the mapping rules and calculates their selectivity on the fly. Then, taking this selectivity into account Morph-CSV decides to create, or not, an index over these columns. Additionally, the mapping document is translated so that it is aligned with the RDB schema that has been created. Figure \ref{fig:rdb} shows the RDB schema generated by Morph-CSV for the input query in Figure \ref{fig:selectionq}, with the applied domain and integrity constraints. 

\begin{figure}[t!]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/steps/rdb.pdf}
    \caption[Output RDB schema from Morph-CSV]{\textbf{Generated schema.} The schema generated by Morph-CSV extracting domain and integrity constraints from the annotations and based on the identified sources selected from the input query.}
    \label{fig:rdb}
\end{figure}


\paragraph{}
There are two main points that make the contributions of Morph-CSV relevant: (i) it incorporates the steps to the standard OBDA workflow without modifying the rest of the steps, hence, it can also benefit from optimizations in other steps of the workflow like query rewriting (reasoning)~\citep{mora2014kyrie2} or query translation (SPARQL-to-SQL)~\citep{priyatna2014formalisation}, and (ii) the reliance of the approach on declarative and standard annotations for OBDA allows the generalization of the proposed steps, usually solved in an ad-hoc manner, not only automatizing the process but also improving its maintainability, understandability and readability.




\subsection{Experimental Evaluation}

This section reports on the results of the empirical evaluation conducted to test the effect of respecting constraints, on the fly, during OBDA query translation over tabular data. Aligned with the defined hypotheses H1 and H2 in Chapter \ref{chap:objectives}, our aim is to answer the following research questions: \textbf{RQ1:} What is the effect of combining different types of constraints over a tabular dataset? \textbf{RQ2:} What is the impact of the constraints when the tabular dataset size increases? \textbf{RQ3:} What is the effect of different kinds of SPARQL query shapes in the extraction and application of constraints?. To answer these questions, we have performed three evaluations in different domains: e-commerce, transportation, and biology. Our first evaluation is in the e-commerce domain, in which we used the Berlin SPARQL Benchmark (BSBM)~\citep{bizer2009berlin}. Our second evaluation is in the transportation domain in which we used the GTFS-Madrid-Bench (Section \ref{chapter5:sec-bench}). Finally, our third evaluation is in the domain of biological data, in which we extend one of our previous proposals~\citep{iglesias2019enhancing} for the generation of an OBDA layer over Bio2RDF tabular datasets. Appendix \ref{apppendix:queries} presents the features of the queries together with the constraints and number of sources used by Morph-CSV. In all of the evaluations the common configurations are:

\noindent\textbf{Engines.} The baselines of our study are two open source SPARQL-to-SQL OBDA engines: Ontop\footnote{\url{https://github.com/ontop/ontop}}$^,$\footnote{We modified the default configuration of Ontop extending the maximum used memory from 512Mg to 8Gb} v3.0.1 and Morph-RDB v3.9.15\footnote{\url{https://github.com/oeg-upm/morph-rdb}}. We select these two engines as they are open source engines (others such as Ultrawrap~\citep{sequeda2013ultrawrap} are not openly available) and also the ones that incorporate the set of most relevant optimizations in the SPARQL-to-SQL query translation process~\citep{priyatna2014formalisation,rodriguez2015efficient}.  To evaluate the baseline approach, we manually generate the relational database schemes of each benchmark without any kind of constraints, and measure the load and query execution times. In order to measure the impact of the additional steps proposed by Morph-CSV\footnote{\url{https://doi.org/10.5281/zenodo.3731941}}$^,$\footnote{\url{https://github.com/oeg-upm/morph-csv}}, we integrate our solution on top of the two OBDA engines in two different configuration: Morph-CSV$^-$ that does not include the source selection step, hence, it loads and applies all the constraints over the input data source each time a query has to be answered, and Morph-CSV that implements the full proposed workflow\footnote{We name the combined engines as follows: a) Morph-CSV: Morph-CSV+Morph-RDB, and Morph-CSV+Ontop; b) Morph-CSV$^-$: Morph-CSV$^-$+Morph-RDB, and Morph-CSV$^-$+Ontop}. To ensure the reproducibility of the experiments, we also provide all of the resources in a docker image.

\noindent\textbf{Metrics.} We measure the loading time of each query and the total query execution time (including the steps proposed by Morph-CSV or baseline when appropriate), and the number of answers obtained (see Appendix \ref{appendix:completeness}). Additionally, we detail the times of each proposed step of our workflow in the execution of each query using Morph-CSV in both configurations (see Appendix \ref{appendix:loadingtime}) following the recommendations proposed in the GTFS-Madrid-Bench~\citep{chaves2020gtfs}. Each query was executed 5 times with a timeout of 1 hour in cold mode, that means that the corresponding database is generated each time a query is going to be evaluated in order to ensure up to date number of answers. Regarding the completeness of the queries, both BSBM benchmark and GTFS-Madrid-Bench provide an RDF materialized version of the input sources that has been loaded in a triplestore (Virtuoso in the case) and used as gold standard. To analyze the completeness of each query, we compare the cardinality of the result set of each configuration against the gold standard assuming its correctness. In the case of the Bio2RDF use case, we cannot compare our results with any gold standard as the last dump version of the project~\citep{dumontier2014bio2rdf} is not comparable with the current status of the input sources, as we declare in one of our previous works~\citep{iglesias2019enhancing}. The experiments were run in an Intel(R) Xeon(R) equipped with a CPU E5-2603 v3 @ 1.60GHz 20 cores, 64GB memory and with the O.S. Ubuntu 16.04LTS.



\subsubsection{BSBM}
As we described in Section \ref{chap2:soa-bench}, the Berlin SPARQL Benchmark~\citep{bizer2009berlin} (BSMB) is one the most popular benchmarks in the Semantic Web field that not only tests the performance of RDF triple stores, but also tests approaches that perform SPARQL-to-SQL query translations providing an RDB instance. It is the chosen benchmark to test the capabilities of many state-of-the-art OBDA engines~\citep{priyatna2014formalisation,calvanese2017ontop,mami2019squerall}. 
 
\begin{figure}[!ht]
  \centering
  \subfloat[Loading time for BSBM 45K.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/carga_bsbm_45.pdf}
  \label{fig:bsbmload45}
  }
  \subfloat[Loading time for BSBM 90K.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/carga_bsbm_90.pdf}  
  \label{fig:bsbmload90}
  }
  \qquad
  \subfloat[Loading time for BSBM 180K.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/carga_bsbm_180.pdf}  
  \label{fig:bsbmload180}
  }
  \subfloat[Loading time for BSBM 360K.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/carga_bsbm_360.pdf}
  \label{fig:bsbmload360}
  }
\caption[Loading Time of Tabular Datasets in BSBM.]{\textbf{Loading Time of Tabular Datasets in BSBM.}  Loading time in seconds of the tabular datasets from the BSBM benchmark with number of products 45K, 90K, 180K and 360K. The baseline approach (red columns) and Morph-CSV$^-$ (light green) are constant for each dataset and query, while Morph-CSV (dark green) depends on the query and number of constraints to be applied over the selected sources.}
\label{fig:bsbmload}
\end{figure}


\begin{figure}[!ht]

  \centering
  \subfloat[Total query execution time for BSBM-45.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/execution_bsbm_morph_45.pdf}  
  \label{fig:morphbsbm45}
  }
  \subfloat[Total query execution time for BSBM-90.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/execution_bsbm_morph_90.pdf}  
  \label{fig:morphbsbm90}
  }
  \qquad
  \subfloat[Total query execution time for BSBM-180.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/execution_bsbm_morph_180.pdf}  
  \label{fig:morphbsbm180}
  }
  \subfloat[Total query execution time for BSBM-360.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/execution_bsbm_morph_360.pdf}
  \label{fig:morphbsbm360}
  }
\caption[Query execution Time in BSBM with Morph-RDB]{\textbf{Query execution Time of Tabular Datasets in BSBM with Morph-RDB.} Execution time in seconds of the tabular datasets from the BSBM benchmark with scale values 45K, 90K, 180K and 360K. The baseline Morph-RDB approach (red columns) is compared with the combination of Morph-CSV (dark green) and Morph-CSV$^-$ (light green) together with Morph-RDB.}
\label{fig:morphbsbm}
\end{figure}


\begin{figure}[!ht]
  \centering
  \subfloat[Total query execution time for BSBM-45.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/execution_bsbm_ontop_45.pdf}  
  \label{fig:ontopbsbm45}
  }
  \subfloat[Total query execution time for BSBM-90.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/execution_bsbm_ontop_90.pdf}  
  \label{fig:ontopbsbm90}
  }
  \qquad
  \subfloat[Total query execution time for BSBM-180.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/execution_bsbm_ontop_180.pdf}  
  \label{fig:ontopbsbm180}
  }
  \subfloat[Total query execution time for BSBM-360.]{
  \includegraphics[width=0.48\linewidth]{figures/bsbm/execution_bsbm_ontop_360.pdf}
  \label{fig:ontopbsbm360}
  }
\caption[Query execution Time in BSBM with Ontop]{\textbf{Query execution Time of Tabular Datasets in BSBM with Ontop.} Execution time in seconds of the tabular datasets from the BSBM benchmark with scale values 45K, 90K, 180K and 360K. The baseline Ontop approach (red columns) is compared with the combination of Morph-CSV (dark green) and Morph-CSV$^-$ (light green) together with Ontop.}
\label{fig:ontopbsbm}
\end{figure}

 
\noindent\textbf{Datasets, annotations and queries.} In order to test our proposal we decided to adapt BSBM, extracting the tabular data sources in CSV format from the SQL generated instances. Additionally, we create the corresponding mapping rules in RML and the metadata following the CSVW specification. We measure the loading time of the two proposals (baseline and Morph-CSV) for each query in the benchmark. Since the focus of Morph-CSV is not the improvement of the support of SPARQL features in the query translation process, we only select the queries of the benchmark that include the supported features  by each engine. This means that Morph-RDB will be evaluated over the queries  Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q8, Q9, Q10 and Q12  and Ontop will be evaluated over Q1, Q3, Q4, Q5 and Q10, both of them using the corresponding R2RML mapping document. For the baseline approach we manually create the RDB schema without  constraints.



\noindent\paragraph*{\textit{BSBM Results}}

\noindent\paragraph*{\textit{Loading Time}.}
The results of the load time for each query and dataset size are shown in Figure \ref{fig:bsbmload}. The main difference between baseline and Morph-CSV$^-$ in comparison with Morph-CSV is that while the loading time for the first two methods is constant for each size, Morph-CSV loading time depends on several input parameters such as the query and the number and type of constraints. In the case of Morph-CSV, it could be understandable that the application of a set of constraints over the raw data in order to improve query performance and completeness, would have a negative impact in the loading time. This happens in queries Q8 and Q11, where the number of sources and the application of the constraints (mainly integrity constraints),  impact negatively on the loading time of the data in the RDB instance in comparison with the baseline approach. However, in the rest of the queries, the Morph-CSV steps focus on the selection of constraints, sources and columns, and on exploiting the information in query and mapping rules, improving the loading time for each query in comparison with the baseline loading time. This means that, although the engine is including a set of additional steps during the starting phase of an OBDA system, the application of these steps only over the data that is required to answer the query, has a positive impact in the total query execution time. Additionally, we can observe that Morph-CSV is able to process, apply the different constraints, and generate the corresponding instance of the RDB for any query. In the case of Morph-CSV$^-$, applying all the constraints defined for the whole dataset each time a query has to be answered, has a negative impact in the loading time, obtaining the worst results in the loading phase.

\noindent\paragraph*{\textit{Evaluation Time with Morph-RDB}.} The query execution time using Morph-RDB as the back-end OBDA engine is shown in Figure \ref{fig:morphbsbm}. The first remarkable observation can be seen in query Q5. Although this query contains features supported by Morph-RDB, the engine reports an error when evaluating the query over the database generated by the baseline approach, because it is not able to evaluate the arithmetic expressions in the FILTER clauses. On the contrary, the datatype of each column in the database generated by Morph-CSV (and also Morph-CSV$^-$) is properly defined, making it possible for Morph-RDB to evaluate the query without any problem and obtaining the expected results. Another remarkable difference is in query Q2, which contains a large number of joins, Morph-RDB reports a timeout error for 180K and 360K with the database generated by the baseline approach. However, it is still able to evaluate this query in reasonable time over the databases generated by Morph-CSV and Morph-CSV$^-$. The effect of the application of integrity constraints in the generation of the RDB instance can also be seen in most of the queries (i.e., Q1, Q2, Q3, Q6, Q9, Q10) reducing considerably the query execution time in the database generated by Morph-CSV in comparison with the baseline approach. There are cases (i.e., Q4, Q7, Q12) where the amount of data to retrieve is large, minimizing the effect of the optimizations. Finally, there are cases where optimizations over the indexes cannot be applied (e.g. querying all the properties of a class). We observe this behavior in Q8, in which the difference between the Morph-CSV+Morph-RDB and Morph-RDB approaches is minimal and this behavior is consistent in all size of datasets. In general, Morph-CSV$^-$ obtains worse results than Morph-CSV+Morph-RDB and Morph-RDB alone. The results are understandable as this configuration has to invest time in preparing the full RDB instance for each query, executing many unnecessary steps in comparison with Morph-CSV. However, in some cases the evaluation time is better than the one obtained over the Morph-RDB configuration, where clearly the creation of indexes and integrity constraints play a key role in the performance of the query execution (see Q2).

\noindent\paragraph*{\textit{Evaluation Time with Ontop}.}
The query execution time using Ontop as the back-end OBDA engine is shown in Figure \ref{fig:ontopbsbm}. Like Morph-RDB, Ontop needs the Morph-CSV generated databases to be able to evaluate Q5 due to the arithmetic expressions of its FILTER operators. Additionally, it also fails in Q10 because it cannot process a FILTER with a date value. In the rest of the queries (Q1, Q3, Q4) we can see that the query evaluation time in Ontop with Morph-CSV is lower than the query evaluation time over the baseline database. Note that in larger databases (180K and 360K), Q1 and Q4 can only be evaluated over the databases generated by Morph-CSV. The Morph-CSV$^-$ configuration is also able to answer the queries just as the Morph-CSV standard configuration, but in comparison with this configuration, the performance is being affected due the inclusion of the additional and unnecessary steps.

As mentioned in the Ontop repository page\footnote{\url{https://github.com/ontop/ontop/wiki/MappingDesignTips}}, integrity constraints are essential for the correct behavior of the engine. Although it is out of the scope of this paper, we observe in our experiments that the main reason why Ontop is only able to answer half of the queries in this benchmark, is related to some issues about maintaining the desirable properties~\citep{corcho2019towards} when translating R2RML mapping rules to its own mappings, called OBDA. The engine also fails to evaluate queries with OPTIONAL clauses when there are NULL values in the answers, as they acknowledged, it is possible that this support has not been implemented in the engine~\citep{xiao2018efficient}.

\noindent\paragraph*{\textit{Query completeness.}} In Table \ref{tab:completeness-bsbm} we show the query completeness obtained with the BSBM benchmark. It is important to remark that our intention to use this benchmark is for testing performance capabilities of our proposals, the input sources are extracted from the BSBM relational model, which is a well formed and normalized RDB instance. However, there are still some cases where we identify the need of applying constraints over the relational database, which are Q5 in the evaluation over Morph-RDB and Q5 and Q10 over Ontop. In these cases, the baseline configurations of the engines are not able to answer those queries, not because they do not support a feature of the SPARQL query or  cannot do it on time, but because they cannot perform the correct comparison among different datatypes in the relational database instance. We demonstrate  with the application of Morph-CSV that queries can be answered and  the correct number of results can be obtained. Additionally, thanks to the application of indexes and integrity constraints there are some queries such as Q1 and Q2 that can be answered by Morph-CSV configuration but not by the baseline, which means that thanks to these steps we are ensuring the effectiveness of the optimizations provided by Ontop and Morph-RDB in the SPARQL-to-SQL translation process. 

\begin{figure}[!ht]
  \centering
  \subfloat[Loading time for GTFS-1.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/carga_gtfs_1.pdf}  
  \label{fig:gtfsload1}
  }
  \subfloat[Loading time for GTFS-10.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/carga_gtfs_10.pdf}  
  \label{fig:gtfsload10}
  }
  \qquad
  \subfloat[Loading time for GTFS-100.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/carga_gtfs_100.pdf}  
  \label{fig:gtfsload100}
  }
  \subfloat[Loading time for GTFS-1000.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/carga_gtfs_1000.pdf}
  \label{fig:gtfsload1000}
  }
\caption[Loading Time of Tabular Datasets in GTFS]{\textbf{Loading Time of Tabular Datasets in GTFS.} Loading time in seconds of the tabular datasets from the Madrid-GTFS-Bench with scale values 1, 10, 100 and 1000. The baseline approach (red columns) and Morph-CSV$^-$ (light green) are constant for each dataset and query, while Morph-CSV (dark green) depends on the query and number of constraints to be applied over the selected sources.}
\label{fig:gtfsload}
\end{figure}



\begin{figure}[!ht]
  \centering
  \subfloat[Total query execution time for GTFS-1.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/execution_gtfs_morph_1.pdf}  
  \label{fig:morphgtfs1}
  }
  \subfloat[Total query execution time for GTFS-10.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/execution_gtfs_morph_10.pdf}  
  \label{fig:morphgtfs10}
  }
  \qquad
  \subfloat[Total query execution time for GTFS-100.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/execution_gtfs_morph_100.pdf}  
  \label{fig:morphgtfs100}
  }
  \subfloat[Total query execution time for GTFS-1000.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/execution_gtfs_morph_1000.pdf}
  \label{fig:morphgtfs1000}
  }
\caption[Query execution Time in GTFS with Morph-RDB]{\textbf{Query execution Time of Tabular Datasets in GTFS with Morph-RDB.} Execution time in seconds of the tabular datasets from the Madrid-GTFS-Bench with scale values 1, 10, 100 and 1000.  The baseline Morph-RDB approach (red columns) is compared with the combination of Morph-CSV (dark green) and Morph-CSV$^-$ (light green) together with Morph-RDB.}
\label{fig:morphgtfs}
\end{figure}

\begin{figure}[!ht]

  \centering
  \subfloat[Total query execution time for GTFS-1.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/execution_gtfs_ontop_1.pdf}  
  \label{fig:ontop1}
  }
  \subfloat[Total query execution time for GTFS-10.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/execution_gtfs_ontop_10.pdf}  
  \label{fig:ontop10}
  }
  \qquad
  \subfloat[Total query execution time for GTFS-100.]{
  \includegraphics[width=0.48\linewidth]{figures/gtfs/execution_gtfs_ontop_100.pdf}  
  \label{fig:ontop100}
  }
  \subfloat[Total query execution time for GTFS-1000.]{  
  \includegraphics[width=0.48\linewidth]{figures/gtfs/execution_gtfs_ontop_1000.pdf}
  \label{fig:ontop1000}
  }
\caption[Query execution Time in GTFS with Ontop]{\textbf{Query execution Time of Tabular Datasets in GTFS with Ontop.} Execution time in seconds of the tabular datasets from the Madrid-GTFS-Bench with scale values 1, 10, 100 and 1000. The baseline Ontop approach (red columns) is compared with the combination of Morph-CSV (dark green) and Morph-CSV$^-$ (light green) together with Ontop.}
\label{fig:ontopgtfs}
\end{figure}



\subsubsection{GTFS-Madrid-Bench}
Presented in Section \ref{chapter5:sec-bench}, we use the GTFS-Madrid-Bench to test the capabilities of our proposed workflow over the tabular distributions provided by the benchmark.

\noindent\textbf{Datasets, annotations and queries.} We select the tabular sources of this benchmark (i.e., the CSV files) and we scale up the original data in several instances (scale factors 10, 100 and 1000). Like our previous evaluation with BSBM benchmark, we only select the queries with features that are supported by each engine: Morph-RDB will be evaluated using  queries Q1, Q2, Q4, Q6, Q7, Q9, Q12, Q13, Q14, Q17 and Ontop will be evaluated using  queries Q1, Q2, Q3, Q4, Q5, Q7, Q9, Q13, Q14, Q17. The description and features of each query are also available online\footnote{\url{https://github.com/oeg-upm/gtfs-bench/}}.

\noindent\paragraph*{\textbf{Madrid-GTFS-Bench Results}}

\noindent\paragraph*{\textit{Loading Time}.}
The loading time of the GTFS-Madrid-Bench queries is shown in Figure \ref{fig:gtfsload}. For GTFS-1 the baseline approach clearly has better performance than Morph-CSV. However, when the size of the datasets increases, the positive effects of applying constraints become more apparent. For most of the queries, the loading time needed by Morph-CSV is lower in comparison to the loading time in the baseline approach. Additionally, similarly to BSBM, there are a set of queries where the application of integrity constraints has a negative impact on the loading time (queries Q1 and Q9). The impact of the application of all of the constraints for answering each query, presented by the configuration Morph-CSV$^-$, clearly impacts over the performance in the loading time.

\noindent\paragraph*{\textit{Evaluation Time with Morph-RDB}.}
The query execution time with Morph-RDB as the back-end OBDA engine is shown in Figure \ref{fig:morphgtfs}. Analyzing the results, we generally observe that the incorporation of Morph-CSV in the workflow of OBDA enhances query performance. With respect to the results of each query, we can observe that on the one hand the behavior of the engine over simple queries (Q1, Q2, Q7, Q12 and Q17) is similar. This is understandable as the selected data sources needed to answer the query do not include the application of several constraints (e.g. there are no joins in the query). On the other hand, in the case of complex queries such as Q4, Q6, Q9, Q13 and Q14, where several tabular sources are needed to answer the queries, the application of constraints has a better impact in comparison to the the baseline approach. Similar behavior is shown over Morph-CSV$^-$, where the complexity of the GTFS data model, with many sources, columns and relations among them, has a clear impact on the total execution time of each query, obtaining worse performance than the baseline in most of the cases. However, for example, in the case of query Q9, Morph-RDB is not able to evaluate the query over the 10th scale database generated by the baseline approach, while in the case of the database generated by Morph-CSV and Morph-CSV$^-$, the query can be answered in reasonable time. In general, due the complexity of GTFS model, we can observe that for small datasets (GTFS-1), the cost of applying the proposed steps of Morph-CSV impacts total execution time. However, when the size of the dataset increases, the baseline approach is impacted due to the fact that it has to load all of the input data sources in the RDB before executing the query, low performance is reported for GTFS-100 and GTFS-1000, including timeout in some queries of the latter. Thanks to the application of the constraints and to the source selection step, for Morph-CSV together with Morph-RDB, the return of the results of the queries has a high performance most of the time. In the cases where Morph-CSV reports a timeout (e.g., Q1 in GTFS-1000); it is because the extremely high number of obtained results cannot be handle by Morph-RDB.

\noindent\paragraph*{\textit{Evaluation Time with Ontop}.}
The experimental evaluation of the query execution in Ontop as the back-end OBDA engine is shown in Figure \ref{fig:ontopgtfs}. This engine is more strict with datatypes in the RDB in comparison with Morph-RDB, and it is why Q2, Q5, Q7 and Q9 produce a failure in the execution over the databases generated by the baseline approach. All these queries have a FILTER clause on a specific datatype (e.g., date, integer, etc) and Ontop proceeds to check the domain constraints before executing the queries. Morph-CSV solves this problem by exploiting the annotations from the metadata and defines the correct datatypes of each column before evaluating the query. For the queries that can be answered by both approaches (Q1, Q3, Q4, Q13, Q14, Q17), the absence of integrity constraints has a negative impact in Ontop, resulting in lower execution time over the databases generated by Morph-CSV. However, similar to the evaluation over Morph-RDB, the complexity of the GTFS data model with a larque quantity of domain and integrity constraints to be applied over the whole dataset, makes that the behavior observed over Morph-CSV$^-$ is being impacted, hence, obtaining worse results that Morph-CSV configuration and the baseline in most of the cases. Finally, in the case where Ontop is not able to evaluate the query under the defined threshold, we report it as a timeout.

\noindent\paragraph*{\textit{Query completeness.}} In the same manner as BSBM benchmark, the focus of the GTFS-Madrid-Bench is on testing the performance and scalability issues of virtual OBDA and OBDI engines. The input dataset is also well formed and normalized. The completeness results of the evaluation are shown in Table \ref{tab:complete-gtfs}, where as we describe before, Morph-RDB has a mechanism to infer the datatypes of the database using the \textit{rr:dataType} annotation from R2RML, which allows  the engine to answer the queries of this benchmark without the need of applying  datatype constraints over the RDB instance. However, Ontop does not include such a mechanism and it needs the declaration of the correct datatypes over the RDB instance, which has a negative impact in the execution of many queries of the benchmark, that cannot be answered using the baseline database but they retrieve the correct results including Morph-CSV (or Morph-CSV$^-$) in the pipeline.

\begin{figure}[t!]
    \centering
    \subfloat[\textbf{Query execution Time of Tabular Datasets in Bio2RDF with Morph-RDB.} Execution time in seconds of the tabular datasets from  Bio2RDF of Morph-CSV and Morph-CSV$^-$ using Morph-RDB as back-end engine. The baseline is not reported as the loading over the RDB instance reports an error.]{
    \includegraphics[width=0.48\linewidth]{figures/bio2rdf/execution_bio2rdf_morph.pdf}
    }
    \subfloat[\textbf{Query execution Time of Tabular Datasets in Bio2RDF with Ontop.} Execution time in seconds of the tabular datasets from  Bio2RDF of Morph-CSV and Morph-CSV$^-$ using Ontop as back-end engine. The baseline is not reported as the loading over the RDB instance reports an error.]{
    \label{fig:bio2rdfmoprh}
    \includegraphics[width=0.48\linewidth]{figures/bio2rdf/execution_bio2rdf_ontop.pdf}
    \label{fig:bio2rdfontop}
    }
    \caption[Query execution Time of Tabular Datasets in Bio2RDF]{Query execution Time of Tabular Datasets in Bio2RDF}
\end{figure}


\subsubsection{The Bio2RDF project}
Bio2RDF is one of the most popular projects that integrates and publishes biomedical datasets as Linked Data~\citep{belleau2008bio2rdf}. Its community has actively contributed to the generation of those datasets using ad-hoc programming scripts, such as PHP. In comparison with the other benchmarks where the focus of the evaluation was the improvement of the query evaluation time, this real use case contains multiple heterogeneity challenges that, for example, enforce the application of ad-hoc transformation functions (i.e., mappings in the form of RML+FnO). Thus, with this use case we want to demonstrate the benefits of exploiting declarative annotations (metadata and mappings) over the raw data in order to improve query completeness and the need of incorporating the proposed steps for executing queries over real world data sources.

\noindent\textbf{Dataset, annotations, and queries.} Tabular datasets in  CSV or Excel formats cover over 35\% of the total datasets in the Bio2RDF project~\citep{iglesias2019enhancing}. In order to test the capabilities of Morph-CSV, we select a subset of the tabular datasets guaranteeing that they cover all of the identified challenges. Additionally, as far as we are aware, there is no standard benchmark over the Bio2RDF project; we also propose a set of SPARQL queries in order to exploit the selected data. Their main features are shown in Appendix \ref{apppendix:queries}). 


\noindent\paragraph*{\textbf{Bio2RDF Results}}
The results obtained for query evaluation in Bio2RDF are shown in Figure \ref{fig:bio2rdfmoprh} with Morph-RDB as back-end engine and in Figure \ref{fig:bio2rdfontop} with Ontop. The detailed results obtained by Morph-CSV and Morph-CSV$^-$ are shown in Table \ref{tab:bio2rdfdeatiledresults} and the completeness in Table \ref{tab:completness-bio2rdf}. Analyzing the obtained results, we can observe that there are no results for the baseline approach, this means it was not possible to create an RDB schema and load the input data manually. The main reasons are the heterogeneity problems of a real use case that do not exist in the previous evaluations. GTFS and BSBM have well formed and standard source data models. Problems such as the absence of column names, multiple formats of same datatype in different files (numbers, dates) and the use of delimiters inside the column data, make it impossible to generate the baseline approach without a manual and ad-hoc pre-processing step. However, exploiting declarative annotations, Morph-CSV is able to apply the proposed workflow to this dataset, and successfully answer the proposed queries with both back-end OBDA engines. Similar to the previous benchmarks, loading the complete dataset for answering the input query (Morph-CSV$^-$ configuration) has an negative impact on the total execution time. We can observe that for the proposed queries, most of the total evaluation time of each query is spent in the loading process, as the total execution time in Morph-CSV$^-$ is pretty similar for all the queries. Contrary, query execution is benefited by this previous step obtaining the results in reasonable time for all of the queries. 


\subsubsection{Discussion of Experimental Results}

We have run an experimental evaluation to analyze what are the effects on the use of declarative annotations to extract and apply constraints to enhance virtual OBDA approaches. We have tested our approach over three different cases: (i) a well known benchmark (BSBM) from the e-commerce domain; (ii) a benchmark focused on a virtual OBDA approach for the transport domain; and (iii) a real use case from the biological domain. We describe the main conclusions and findings based on the results obtained:
\begin{itemize}
    \item \textbf{Query complexity:} Clear benefits are obtained from being able to analyze and take advantage of the information provided by the input query, before translating and running it. It allows to only select sources and constraints that are going to be useful for answering the query, avoiding carrying out additional and unnecessary functions over the raw data. Together with the mapping rules, the queries are essential to make relevant decisions during the on-the-fly physical design of the RDB instance (e.g., integrity constraints). Approaches such as the Morph-CSV$^-$ configuration can be valuable when the freshness of the results is not a main requirement, for example to perform a materialization process, which will ensure high quality RDF files where the domain constraints have been applied.
    \item \textbf{Data size:} The total query evaluation time is being impact from how the engine manages the input dataset and the application of constraints. The delegation of these operations to the RDBMS system after loading the full dataset may not be efficient enough. Morph-CSV pushes down the source selection and the application of domain constraints over the raw data. Although it incorporates a set of additional steps in comparison with the baseline, the benefits in the query execution time by the SPARQL-to-SQL engine are already demonstrated, enhancing the total execution time of the queries in most of the cases.
    \item \textbf{Declarative annotations:} The use of declarative and standard mapping rules and metadata makes it possible the generalization of the proposal, avoiding ad-hoc and manual steps. It also incorporates a set of important benefits for the process such as the improvement of its maintainability, readability, and understandability.
    \item \textbf{Querying raw data in OBDA:} Most of the data shared on the web is currently raw data in well known formats such as CSV, JSON, and XML. Semantic Web and more specifically, OBDA technologies, play a key role in starting to see the web as an integrated  database that can be queried. With this approach, we demonstrate that querying tabular data is: i) neither a trivial nor an easy task that can be delegated to na\"ive querying approaches and ii) optimizations and improvements can still be proposed taking advantage and exploiting current annotation proposals to not only enhance performance but also completeness.
\end{itemize}


\subsection{Conclusions}
In this section, we have presented an extension of the common OBDA specification to address the problem of query translation over tabular data. We describe and evaluate Morph-CSV, a framework that exploits the information of mapping rules and metadata OBDA annotations to extract and apply a set of relevant constraints. It pushes down the application of these elements directly over the raw data in order to improve query evaluation and query completeness. One of the main contributions of this proposal is that it can be used together with any OBDA framework. From the set of experiments that we have performed with two existing state-of-the-art OBDA engines (Morph-RDB and Ontop), we can see that the use of those engines inside the Morph-CSV framework brings several positive impacts: more queries can be answered and less time is needed to answer most queries.

The definition, application and optimization of new functions and constraints to address other challenges for querying tabular data is one of the main lines for future work~\citep{iglesias2019enhancing}. We also want to study the performance of the proposed workflow over OBDA distributed query systems such as the ones proposed in~\citep{endris2019ontario,mami2019squerall}. More in detail, we want to analyze if the outcomes of the proposed steps by Morph-CSV can help in distributed environments where physical design of knowledge graphs are being proposed~\citep{rohde2020optimizing,upm63647} to enhance query performance (i.e., deciding which input sources have to be transformed to RDF and which ones have to be maintained in their original format). Additionally, one of the possible future work lines is the comparison of the proposed approached, that exploits semantic web technologies and annotations, against non-semantic web solutions that provide support to deal with the identified challenges for querying tabular data (e.g., Apache Drill, Presto, Spark, etc.). The results obtained can also be useful to machine learning approaches that identify when the application of the integrity constraints is needed or not, as we observe that there are special cases where it can have a negative impact. We will also study the challenges for querying other data formats (e.g., XML, JSON) in an OBDA context and extend our approach to incorporate them. We also want to remark the importance of having standard and shared methods and vocabularies to publish metadata of raw data on the web. JSON or XML Schema should be included in the construction of knowledge graphs when these formats are used for the input sources. Finally, we will adapt this proposal for a materialization process and study its effects comparing it with previous proposals. We also want to study how the materialization process can be improved when historical versions of the same RDF-based knowledge graph are needed, for example, analyzing which input sources have been changed or not, to decide which parts of that knowledge graph have to be generated again.




